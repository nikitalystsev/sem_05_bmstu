ИУ7-53Б, 11\_KNY, Лысцев Никита

\chapter{Статистический машинный перевод. Методы статистического машинного перевода}

% решаемая МП задача

В современном мире существует большое количество статей, технической документации, книг, написанных на разных языках. Перевод таких текстов с одного языка на другой человеком вручную требует значительных временных и финансовых затрат, а также наличия соответствующих навыков. Возникает проблема перевода. Решением данной проблемы может стать максимальная автоматизация этого процесса. С этим помогают системы машинного перевода (МП).

% определение СМП и языковых пар

\section{Статистический машинный перевод}

Статистический машинный перевод (СМП) -- разновидность МП, которая основана на поиске наиболее вероятного перевода предложения с использованием данных, полученных из двуязычной совокупности текстов (параллельного корпуса) в результате  обучения (по языковым парам)\cite{smt}.

При статистическом подходе используется такое понятие как <<канал   помехами>>(Noisy-Channel Model)\cite{smt_book}: рассматривая перевод с английского на русский, предполагается, что английское предложение на самом деле – русское предложение, но искаженное неким шумом. Для корректного перевода или же <<расшифровки шума>> необходимо знать, что в рассматриваемом случае говорят русскоязычные и как рассматриваемый текст искажается, что превращается в текст на английском языке. Перевод осуществляется после поиска такого русского предложения, которое максимизирует вероятность «встречи» английского предложения и соответствующего ему русского предложения. В основе такого поиска лежит теорема Байеса:

\begin{equation}
	\label{eq:L}
	arg max_e P(e|f) = arg max_e P(f|e) \cdot P(e)
\end{equation}

где e -- предложение перевода, f -- предложение оригинала.

Таким образом, в основе перевода, то есть вероятности того, что
предложение оригинала $f$ будет переведено конечным предложением $e$, лежат модель языка ($P(e)$ в формуле \ref{eq:L}), и модель перевода -- ($P(f|e)$ в формуле \ref{eq:L}). 

\clearpage

Модель языка должна присваивать оценку вероятности любому предложению конечного языка, а модель перевода должна присваивать оценку вероятности предложения оригинала при условии определенного предложения на конечном языке \cite{smt}. 

Таким образом, задача, которая ставится перед системой статистического машинного перевода – не перевод, как таковой, а декодирование. То есть, в данном случае, перевод -- не что иное, как расшифровка исходного текста. 

Формально, алгоритм работы системы статистического машинного перевода можно описать в следующих шагах:

\begin{enumerate}
	\item Составляются параллельные корпусы \cite{smt} для языка источника и языка перевода;
	\item Производится выравнивание \cite{smt} параллельных корпусов текстов;
	\item Согласно выбранной модели перевода ищутся такие значения таблиц переводных соответствий \cite{smt_book}, которые максимизируют вероятность части корпуса источника при имеющейся части корпуса языка перевода;
	\item На основе корпуса языка перевода составляется модель языка.
	\item На основе полученных данных для незнакомого предложения на языке источника ищется предложение на языке перевода, максимизирующее произведение вероятностей, присваеваемых моделью языка и моделью перевода.
\end{enumerate}

\clearpage

Алгоритм перевода можно изобразить следующей схемой.

\includesvgimage
{algSMT} % Имя файла без расширения (файл должен быть расположен в директории inc/img/)
{f} % Обтекание (без обтекания)
{h} % Положение рисунка (см. figure из пакета float)
{1\textwidth} % Ширина рисунка
{Алгоритм работы системы статистического машинного перевода} % Подпись рисунка

Итак, весь алгоритм основывается на выравнивании, создании моделей
и самом переводе. Рассмотрим данные составляющие более подробно.

\subsection{Выравнивание параллельных корпусов}

Параллельный текст -- текст на одном языке вместе с его переводом на другой язык. Параллельный корпус -- большие собрания параллельных текстов \cite{smt}.

Выравнивание параллельных корпусов -- процесс, в котором для каждого параллельного текста каждому его предложению на языке источника ставится в соответствие предложение на языке перевода. 

Для выравнивания текстов система использует инструментальное средство
автоматического выравнивания параллельных текстов (alignment tool). С
помощью таких инструментов система может проанализировать и привести в
соответствие тексты оригинала и перевода \cite{smt2}. 

Подготовленные таким образом тексты используются для обучения модели перевода.

\subsection{Модель перевода}

Модель перевода, или таблиц перевода – это таблица-словарь, в которой для всех известных системе слов и фраз на одном языке перечислены все возможные их переводы на другой язык и указана вероятность этих переводов \cite{smt2}.

\section{Модель языка}

Модель языка оценивает фразы целевого языка и дает им соответствующую вероятность. В качестве модели языка в системах статистического машинного перевода используются преимущественно используются различные модификации $n$ - граммной модели, утверждающей, что грамматичность выбора очередного слова определяется лишь тем, какие $(n - 1)$ слов идут перед ним. Вероятность каждого $n$ - грамма определяется по его встречаемости в параллельном корпусе\cite{smt}.

Рассмотрим  математику $n$ - граммных моделей с $n = 3$:

\begin{equation}
	\label{eq:ngram}
	\begin{gathered}
		P(e) = P(e_1, e_2, \ldots, e_n) = \\
		P(e_1) \cdot P(e_2 | e_1) \dotsb P(e_n | e_1, e_2, \ldots , e_{n-1}) \simeq \\
		P(e_1) \cdot P(e_2 | e_1) \dotsb P(e_n | e_{n-2},e_{n-1})
	\end{gathered}
\end{equation}

где $e$ -- предложение на языке перевода, $P(e)$  -- вероятность перевода всего предложения $e$, $e_i$ -- слово в предложении $e$, $i = \overline{1, n}$,   $P(e_i)$ -- вероятность перевода слова $e_i$, $i = \overline{1, n}$.

\subsection{Декодер}

Декодер – составляющая переводчика, которая непосредственно переводом. Для каждого предложения исходного текста он подбирает все варианты перевода, сочетая между собой фразы из модели перевода, и сортирует их по убыванию вероятности. Затем все получившиеся варианты декодер
оценивает с помощью модели языка. 

\clearpage

\section{Методы статистического машинного перевода}

В статистическом машинном переводе существует два основных метода перевода:

\begin{itemize}[label*=--]
	\item метод перевода, основанный на словах;
	\item метод перевода, основанный на фразах.
\end{itemize}

Каждому из вышеперечисленных методов соответствуют свои модели перевода. Рассмотрим каждый из методов более подробно.

\subsection{Метод перевода, основанный на словах}

Введем некоторые обозначения:

\begin{itemize}[label*=--]
	\item $f$ -- слово на языке источника, с которого нужно переводить;
	\item $e$ -- слово на языке приемника, на который нужно переводить.
\end{itemize}

Понятие статистического машинного перевода подразумевает использование статистики. На основе анализа параллельных корпусов для каждого слова $f$ в предложении выполняется подсчет количества переводов для каждого возможного варианта перевода $e$ на язык приемника. Далее, на основе этих подсчетов, производится оценка распределения вероятности лексического перевода. 

Формально, находится функция \cite{smt_book}

\begin{equation}
	\label{eq:prob}
	p_f : e \rightarrow p_f(e)
\end{equation}

что для слова $f$ возвращает вероятность для каждого выбора перевода $e$, которая указывает, насколько вероятен этот перевод.

Функция \ref{eq:prob} должна  обладать следующим свойством:

\begin{equation}
	\label{eq:prop1_prob}
	\displaystyle\sum_e p_f(e) = 1
\end{equation}

После получения вероятностного распределения для каждого слова $f$ в предложении выбирается наиболее вероятный перевод $e$. Сопоставление слова $f$ и его перевода $e$ в предложении на языках источника и приемника определяются функцией выравнивания:

\begin{equation}
	\label{eq:alig}
	a : j \rightarrow i
\end{equation}

которая отображает каждое выходное слово в позиции $i$ к входному слову в позиции $j$.

\subsubsection*{IBM Model 1}

Генеративный метод моделирования -- метод разбиение процесса генерации данных на более мелкие шаги, моделирование более мелких шагов с помощью вероятностных распределений и объединение шагов в последовательную историю \cite{smt_book}.

Поскольку прямое моделирование распределения вероятностей перевода для полных предложений сложно оценить (большинство предложений встречается только один раз, даже в больших текстовых коллекциях), применяется генеративный метод моделирования: процесс разбивается на более мелкие этапы, в нашем случае на перевод отдельных слов, а их расстановку в правильном порядке обеспечит модель языка.

IBM Model 1 -- генеративная модель перевода предложений, основанная исключительно на распределении вероятностей лексического перевода \cite{smt_book}. Данная модель генерирует несколько различных переводов предложения, каждый из которых имеет разную вероятность. Единственным массивом данных, которым оперирует IBM Model 1 является таблица вероятностей попарно переводимых соответствий слов двух языков \cite{smt}.

Обучение IBM Model 1 производится на параллельных корпусах, выровненных н уровне предложений. IBM Model 1 допускает ситуацию, в которой наиболее употребительным переводом нескольких смысловых слов может быть признано одно высокочастотное -- например, служебное -- слово конечного языка, и данная модель не всегда правильно учитывает порядок слов в предложении \cite{smt}.

\subsubsection*{IBM Model 2}

Чтобы сохранить при переводе информацию, заключенную в порядке слов, была предложена IBM Model 2. 

В IBM Model 2 помимо таблица вероятностей попарно переводимых соответствий слов двух языков вводится таблица распределения вероятностей  выравнивания, то есть вероятностей, что при определенной длине предложения на языке перевода $l_e$ и длине предложения на языке источника $l_f$ отображает каждое выходное слово в позиции $i$ к входному слову в позиции $j$.

Таким образом, перевод с помощью IBM Model 2  можно описать в двух этапах:

\begin{enumerate}
	\item Выполнение лексического перевода, как в IBM Model 1;
	\item Выполнение этапа выравнивания.
\end{enumerate}

\subsection{Метод перевода, основанный на фразах}

Наиболее эффективные в настоящее время системы статистического машинного
перевода основаны на моделях, основанных на фразах: моделях, которые переводят небольшие последовательности слов за раз \cite{smt_book}.

Под фразой понимается любая многословная единица предложения \cite{smt_book}. В методе перевода, основанном на фразах, наименьшей единицей перевода является не слово, как в первом методе, а фраза. 

Алгоритм перевода следующий:

\begin{enumerate}
	\item Входное предложение на языке источника разбивается на фразы;
	\item Каждая фраза переводится на фразу в языке перевода;
	\item Фразы на языке перевода переупорядочиваются согласно с моделью языка.
\end{enumerate}

В основе перевода на основе фраз лежит правило Байеса \cite{smt_book}:

\begin{equation}
	\label{eq:L}
	e_{best} = arg max_e P(e|f) = arg max_e P(f|e) \cdot P_{LM}(e)
\end{equation}

где $e_{best}$ -- наиболее вероятный перевод предложения $f$ на языке источника, $P_{LM}(e)$ -- модель языка, $P(f|e)$ -- модель перевода.

Данный метод имеет несколько преимуществ. Во-первых, слова, возможно, не являются лучшим атомарным средством единиц для перевода из-за частых сопоставлений один ко многим (и наоборот).Во-вторых, перевод групп слов вместо отдельных слов помогает устранить неоднозначность перевода. Есть и третье преимущество: если у нас есть большие учебные корпуса, мы можем выучить все более и более длинные полезные фразы, иногда даже запоминать перевод целых предложений \cite{smt_book}.

\subsubsection*{IBM Model 3}

IBM Model 2 не допускает возможности, что  одному слову из предложения на языке источника соответствует соответствует несколько слов в предложении на языке перевода. Этот недостаток устраняется в IBM Model 3, где вводится понятие коэффициента деления слова оригинала, и, соответственно, таблица вероятностей каждого значения коэффициента  деления для каждого слова \cite{smt}.

\subsubsection*{IBM Model 4 и IBM Model 5}

В IBM Model 4 и близкой к ней IBM Model 5 делается следующий шаг к включению понятия грамматики в систему статистического машинного перевода. В IBM Model 4 появляется понятие класса слов, определяемое автоматически для всех слов языка оригинала и языка перевода.